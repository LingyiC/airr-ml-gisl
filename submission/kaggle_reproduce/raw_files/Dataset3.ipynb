{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fdcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 320 train, 80 validation (seed=42)\n",
      "Analyzing 160 Positive Patients and 160 Negative Patients.\n",
      "Counting sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positives: 100%|██████████| 160/160 [00:08<00:00, 19.65it/s]\n",
      "Positives: 100%|██████████| 160/160 [00:08<00:00, 19.65it/s]\n",
      "Negatives: 100%|██████████| 160/160 [00:03<00:00, 40.72it/s]\n",
      "Negatives: 100%|██████████| 160/160 [00:03<00:00, 40.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning for features (Min Pos=3, Max Neg=0, P < 0.05)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3357656/3357656 [00:02<00:00, 1127270.06it/s]\n",
      "100%|██████████| 3357656/3357656 [00:02<00:00, 1127270.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "DIAGNOSTIC REPORT\n",
      "Total Unique Sequences Seen: 3357656\n",
      "Discarded (Too Rare in Positives): 3297023\n",
      "Discarded (Too Common in Negatives): 44455\n",
      "Discarded (Not Significant): 15449\n",
      "KEPT (Significant Signals): 729\n",
      "========================================\n",
      "Top 5 Sequences found:\n",
      "     pos_count  neg_count   p_value\n",
      "35          10          0  0.000845\n",
      "118         10          0  0.000845\n",
      "287         10          0  0.000845\n",
      "158         10          0  0.000845\n",
      "90           9          0  0.001740\n",
      "\n",
      "Saved to 'enriched_signatures_strict.pkl'. You can now run Part 2.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_PATH = Path(\"./Documents/airr-ml/data/train_datasets/train_dataset_3/\")\n",
    "METADATA_PATH = BASE_PATH / \"metadata.csv\"\n",
    "SEED = 42\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Lowered thresholds slightly for safety, but kept strict enough to be useful\n",
    "MIN_POS_PATIENTS = 3   # Lowered from 7 to 3\n",
    "MAX_NEG_PATIENTS = 0   # Keep strict: signal shouldn't be in negatives\n",
    "P_VALUE_CUTOFF = 0.05 # Relaxed from 1e-5 to 1e-4\n",
    "\n",
    "# Load Metadata & Split with FIXED SEED\n",
    "df_meta = pd.read_csv(METADATA_PATH)\n",
    "train_df, val_df = train_test_split(\n",
    "    df_meta, test_size=0.2, stratify=df_meta['label_positive'], random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Split: {len(train_df)} train, {len(val_df)} validation (seed={SEED})\")\n",
    "\n",
    "def load_signatures(filename):\n",
    "    path = BASE_PATH / filename\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep='\\t', usecols=['junction_aa', 'v_call', 'j_call'])\n",
    "        return set(zip(df['junction_aa'], df['v_call'], df['j_call']))\n",
    "    except:\n",
    "        return set()\n",
    "\n",
    "# --- COUNTING ---\n",
    "pos_train = train_df[train_df['label_positive'] == True]['filename'].tolist()\n",
    "neg_train = train_df[train_df['label_positive'] == False]['filename'].tolist()\n",
    "n_pos = len(pos_train)\n",
    "n_neg = len(neg_train)\n",
    "\n",
    "print(f\"Analyzing {n_pos} Positive Patients and {n_neg} Negative Patients.\")\n",
    "\n",
    "counts_dict = {}\n",
    "\n",
    "print(\"Counting sequences...\")\n",
    "for fname in tqdm(pos_train, desc=\"Positives\"):\n",
    "    for sig in load_signatures(fname):\n",
    "        if sig not in counts_dict: counts_dict[sig] = [0, 0]\n",
    "        counts_dict[sig][0] += 1\n",
    "\n",
    "# Optimization: Only count negatives if the sequence was seen in positives\n",
    "for fname in tqdm(neg_train, desc=\"Negatives\"):\n",
    "    for sig in load_signatures(fname):\n",
    "        if sig in counts_dict:\n",
    "            counts_dict[sig][1] += 1\n",
    "\n",
    "# --- DIAGNOSTIC FISHER'S TEST ---\n",
    "significant_features = []\n",
    "print(f\"\\nScanning for features (Min Pos={MIN_POS_PATIENTS}, Max Neg={MAX_NEG_PATIENTS}, P < {P_VALUE_CUTOFF})...\")\n",
    "\n",
    "# DIAGNOSTIC COUNTERS\n",
    "stats_counter = {\n",
    "    \"total_checked\": 0,\n",
    "    \"failed_min_pos\": 0,\n",
    "    \"failed_max_neg\": 0,\n",
    "    \"failed_p_value\": 0,\n",
    "    \"passed\": 0\n",
    "}\n",
    "\n",
    "for sig, counts in tqdm(counts_dict.items()):\n",
    "    stats_counter[\"total_checked\"] += 1\n",
    "    pos_with = counts[0]\n",
    "    neg_with = counts[1]\n",
    "    \n",
    "    # Check 1: Frequency Filter\n",
    "    if pos_with < MIN_POS_PATIENTS:\n",
    "        stats_counter[\"failed_min_pos\"] += 1\n",
    "        continue\n",
    "        \n",
    "    # Check 2: Safety Filter (Anti-Noise)\n",
    "    if neg_with > MAX_NEG_PATIENTS: \n",
    "        stats_counter[\"failed_max_neg\"] += 1\n",
    "        continue\n",
    "\n",
    "    pos_without = n_pos - pos_with\n",
    "    neg_without = n_neg - neg_with\n",
    "    \n",
    "    _, p_val = stats.fisher_exact(\n",
    "        [[pos_with, neg_with], [pos_without, neg_without]], \n",
    "        alternative='greater'\n",
    "    )\n",
    "    \n",
    "    # Check 3: Statistical Significance\n",
    "    if p_val >= P_VALUE_CUTOFF:\n",
    "        stats_counter[\"failed_p_value\"] += 1\n",
    "        continue\n",
    "        \n",
    "    # PASSED ALL CHECKS\n",
    "    stats_counter[\"passed\"] += 1\n",
    "    significant_features.append({\n",
    "        'signature': sig,\n",
    "        'p_value': p_val,\n",
    "        'pos_count': pos_with,\n",
    "        'neg_count': neg_with\n",
    "    })\n",
    "\n",
    "# --- REPORTING RESULTS ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"DIAGNOSTIC REPORT\")\n",
    "print(f\"Total Unique Sequences Seen: {stats_counter['total_checked']}\")\n",
    "print(f\"Discarded (Too Rare in Positives): {stats_counter['failed_min_pos']}\")\n",
    "print(f\"Discarded (Too Common in Negatives): {stats_counter['failed_max_neg']}\")\n",
    "print(f\"Discarded (Not Significant): {stats_counter['failed_p_value']}\")\n",
    "print(f\"KEPT (Significant Signals): {stats_counter['passed']}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if len(significant_features) == 0:\n",
    "    print(\"ERROR: No features found. Please RELAX your thresholds.\")\n",
    "    print(\"Try lowering MIN_POS_PATIENTS to 3 or 4.\")\n",
    "else:\n",
    "    enrichment_df = pd.DataFrame(significant_features).sort_values('p_value')\n",
    "    print(\"Top 5 Sequences found:\")\n",
    "    print(enrichment_df.head(5)[['pos_count', 'neg_count', 'p_value']])\n",
    "    \n",
    "    enrichment_df.to_pickle(\"enriched_signatures_strict.pkl\")\n",
    "    print(\"\\nSaved to 'enriched_signatures_strict.pkl'. You can now run Part 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b6819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 729 features.\n",
      "Building Training Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Matrix: 100%|██████████| 320/320 [00:05<00:00, 57.18it/s]\n",
      "Building Matrix: 100%|██████████| 320/320 [00:05<00:00, 57.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Validation Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Matrix: 100%|██████████| 80/80 [00:01<00:00, 58.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Lasso (L1) Model...\n",
      "\n",
      "==============================\n",
      "LASSO RESULTS (Features=729)\n",
      "Validation AUC: 0.7719\n",
      "Validation Acc: 0.6875\n",
      "Active Features Used: 135\n",
      "==============================\n",
      "Avg Prob for Positives: 0.5408\n",
      "Avg Prob for Negatives: 0.2847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_PATH = Path(\"./Documents/airr-ml/data/train_datasets/train_dataset_3/\")\n",
    "METADATA_PATH = BASE_PATH / \"metadata.csv\"\n",
    "SEED = 42\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load Metadata & Split (SAME seed as Cell 1)\n",
    "df_meta = pd.read_csv(METADATA_PATH)\n",
    "train_df, val_df = train_test_split(\n",
    "    df_meta, \n",
    "    test_size=0.2, \n",
    "    stratify=df_meta['label_positive'], \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Load your 897 winning signatures\n",
    "enrichment_df = pd.read_pickle(\"enriched_signatures_strict.pkl\")\n",
    "feature_list = enrichment_df['signature'].tolist()\n",
    "feature_map = {sig: i for i, sig in enumerate(feature_list)}\n",
    "\n",
    "print(f\"Loaded {len(feature_list)} features.\")\n",
    "\n",
    "def load_signatures(filename):\n",
    "    path = BASE_PATH / filename\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep='\\t', usecols=['junction_aa', 'v_call', 'j_call'])\n",
    "        return set(zip(df['junction_aa'], df['v_call'], df['j_call']))\n",
    "    except:\n",
    "        return set()\n",
    "\n",
    "def create_sparse_matrix(filenames, feature_map):\n",
    "    \"\"\"\n",
    "    Creates a (Samples x Features) binary matrix.\n",
    "    Rows = Patients, Cols = Specific Public Sequences.\n",
    "    \"\"\"\n",
    "    n_samples = len(filenames)\n",
    "    n_features = len(feature_map)\n",
    "    \n",
    "    # We use a LIL matrix for efficient construction, then convert to CSR\n",
    "    X = sparse.lil_matrix((n_samples, n_features), dtype=np.int8)\n",
    "    \n",
    "    for row_idx, fname in enumerate(tqdm(filenames, desc=\"Building Matrix\")):\n",
    "        sigs = load_signatures(fname)\n",
    "        for s in sigs:\n",
    "            if s in feature_map:\n",
    "                col_idx = feature_map[s]\n",
    "                X[row_idx, col_idx] = 1\n",
    "                \n",
    "    return X.tocsr()\n",
    "\n",
    "# --- BUILD MATRICES ---\n",
    "print(\"Building Training Matrix...\")\n",
    "X_train = create_sparse_matrix(train_df['filename'], feature_map)\n",
    "y_train = train_df['label_positive'].astype(int).values\n",
    "\n",
    "print(\"Building Validation Matrix...\")\n",
    "X_val = create_sparse_matrix(val_df['filename'], feature_map)\n",
    "y_val = val_df['label_positive'].astype(int).values\n",
    "\n",
    "# --- TRAIN LASSO MODEL ---\n",
    "# penalty='l1' selects only the useful features\n",
    "# C=1.0 is standard; if accuracy is low, try C=0.1 or C=10\n",
    "print(\"Training Lasso (L1) Model...\")\n",
    "clf = LogisticRegression(\n",
    "    penalty='l1', \n",
    "    solver='liblinear', \n",
    "    class_weight='balanced', \n",
    "    C=1.0, \n",
    "    random_state=SEED\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --- EVALUATE ---\n",
    "val_probs = clf.predict_proba(X_val)[:, 1]\n",
    "val_auc = roc_auc_score(y_val, val_probs)\n",
    "val_acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "\n",
    "# Count how many features the model actually used (non-zero weights)\n",
    "n_active_features = np.sum(clf.coef_ != 0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"LASSO RESULTS (Features={len(feature_list)})\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Acc: {val_acc:.4f}\")\n",
    "print(f\"Active Features Used: {n_active_features}\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# DEBUG: Check the \"Score\" distribution\n",
    "# If the model works, Positives should have high scores, Negatives low.\n",
    "pos_scores = val_probs[y_val == 1]\n",
    "neg_scores = val_probs[y_val == 0]\n",
    "print(f\"Avg Prob for Positives: {np.mean(pos_scores):.4f}\")\n",
    "print(f\"Avg Prob for Negatives: {np.mean(neg_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671f08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RUNNING PIPELINE FOR DATASET 3 ---\n",
      "Params: MinPos=3, MaxNeg=0, P=0.05\n",
      "Step 1: Loading Metadata...\n",
      "Training on FULL DATA: 200 Positives, 200 Negatives\n",
      "Step 2: Counting Sequences (Enrichment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positives: 100%|██████████| 200/200 [00:09<00:00, 22.16it/s]\n",
      "Positives: 100%|██████████| 200/200 [00:09<00:00, 22.16it/s]\n",
      "Negatives: 100%|██████████| 200/200 [00:05<00:00, 39.44it/s]\n",
      "Negatives: 100%|██████████| 200/200 [00:05<00:00, 39.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Calculating Statistics & Selecting Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4131240/4131240 [00:03<00:00, 1173544.40it/s]\n",
      "100%|██████████| 4131240/4131240 [00:03<00:00, 1173544.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECTED FEATURES: 1008\n",
      "Signals saved to 'dataset_3_top_signals.csv'\n",
      "Step 4: Building Sparse Training Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:07<00:00, 56.02it/s]\n",
      "100%|██████████| 400/400 [00:07<00:00, 56.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Training Lasso Logistic Regression...\n",
      "Model Trained. Active Features: 155\n",
      "Step 6: Processing Test Data...\n",
      "Found 400 test files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:07<00:00, 56.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUCCESS! Predictions saved to 'dataset_3_submission.csv'\n",
      "                           filename  probability\n",
      "0  00bf3e8b5c44db7f9765a2c896308ca8     0.151802\n",
      "1  00c0a5b0b5780839679ddcede9da56df     0.236472\n",
      "2  00d82591a1ef87f94bc28b47f9a6c9a2     0.632017\n",
      "3  016b2ba917244113cdc4dd790c5ff3c7     0.685355\n",
      "4  02dd6983048af267517bc0a57d53e1b0     0.967874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# --- 1. MASTER CONFIGURATION ---\n",
    "# ==========================================\n",
    "# Change this to 1, 2, or 3 to switch datasets\n",
    "DATASET_NUM = 3 \n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(f\"./Documents/airr-ml/data\")\n",
    "TRAIN_DIR = BASE_DIR / f\"train_datasets/train_dataset_{DATASET_NUM}/\"\n",
    "TEST_DIR = BASE_DIR / f\"test_datasets/test_dataset_{DATASET_NUM}/\"\n",
    "METADATA_PATH = TRAIN_DIR / \"metadata.csv\"\n",
    "\n",
    "# --- PARAMETER TUNING CHEAT SHEET ---\n",
    "# Dataset 3 (Easy): MinPos=5, MaxNeg=1, P=0.01 (or strict MinPos=3, MaxNeg=0, P=0.05)\n",
    "# Dataset 2 (Med):  MinPos=3, MaxNeg=0, P=0.20\n",
    "# Dataset 1 (Hard): MinPos=2, MaxNeg=0, P=1.0 (Reliance on Lasso)\n",
    "\n",
    "if DATASET_NUM == 3:\n",
    "    MIN_POS_PATIENTS = 3\n",
    "    MAX_NEG_PATIENTS = 0\n",
    "    P_VALUE_CUTOFF = 0.05\n",
    "elif DATASET_NUM == 2:\n",
    "    MIN_POS_PATIENTS = 3\n",
    "    MAX_NEG_PATIENTS = 0\n",
    "    P_VALUE_CUTOFF = 0.20\n",
    "elif DATASET_NUM == 1:\n",
    "    MIN_POS_PATIENTS = 2\n",
    "    MAX_NEG_PATIENTS = 0\n",
    "    P_VALUE_CUTOFF = 1.0\n",
    "# ==========================================\n",
    "\n",
    "print(f\"--- RUNNING PIPELINE FOR DATASET {DATASET_NUM} ---\")\n",
    "print(f\"Params: MinPos={MIN_POS_PATIENTS}, MaxNeg={MAX_NEG_PATIENTS}, P={P_VALUE_CUTOFF}\")\n",
    "\n",
    "def load_signatures(filepath):\n",
    "    \"\"\"Efficiently loads unique (Junction, V, J) tuples\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep='\\t', usecols=['junction_aa', 'v_call', 'j_call'])\n",
    "        return set(zip(df['junction_aa'], df['v_call'], df['j_call']))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- STEP 1: LOAD FULL METADATA ---\n",
    "print(\"Step 1: Loading Metadata...\")\n",
    "df_meta = pd.read_csv(METADATA_PATH)\n",
    "pos_files = df_meta[df_meta['label_positive'] == True]['filename'].tolist()\n",
    "neg_files = df_meta[df_meta['label_positive'] == False]['filename'].tolist()\n",
    "\n",
    "print(f\"Training on FULL DATA: {len(pos_files)} Positives, {len(neg_files)} Negatives\")\n",
    "\n",
    "# --- STEP 2: FEATURE ENRICHMENT (FULL DATA) ---\n",
    "print(\"Step 2: Counting Sequences (Enrichment)...\")\n",
    "counts_dict = {}\n",
    "\n",
    "# Count Positives\n",
    "for fname in tqdm(pos_files, desc=\"Positives\"):\n",
    "    for sig in load_signatures(TRAIN_DIR / fname):\n",
    "        if sig not in counts_dict: counts_dict[sig] = [0, 0]\n",
    "        counts_dict[sig][0] += 1\n",
    "\n",
    "# Count Negatives (Optimization: Only check if seen in positives)\n",
    "for fname in tqdm(neg_files, desc=\"Negatives\"):\n",
    "    for sig in load_signatures(TRAIN_DIR / fname):\n",
    "        if sig in counts_dict:\n",
    "            counts_dict[sig][1] += 1\n",
    "\n",
    "# Perform Fisher's Test\n",
    "print(\"Step 3: Calculating Statistics & Selecting Features...\")\n",
    "significant_features = []\n",
    "n_pos = len(pos_files)\n",
    "n_neg = len(neg_files)\n",
    "\n",
    "for sig, counts in tqdm(counts_dict.items()):\n",
    "    pos_with = counts[0]\n",
    "    neg_with = counts[1]\n",
    "    \n",
    "    # FILTER 1: Frequency\n",
    "    if pos_with < MIN_POS_PATIENTS: continue\n",
    "    \n",
    "    # FILTER 2: Exclusivity (Safety)\n",
    "    if neg_with > MAX_NEG_PATIENTS: continue\n",
    "    \n",
    "    # FILTER 3: P-Value\n",
    "    pos_without = n_pos - pos_with\n",
    "    neg_without = n_neg - neg_with\n",
    "    _, p_val = stats.fisher_exact(\n",
    "        [[pos_with, neg_with], [pos_without, neg_without]], \n",
    "        alternative='greater'\n",
    "    )\n",
    "    \n",
    "    if p_val < P_VALUE_CUTOFF:\n",
    "        significant_features.append(sig)\n",
    "\n",
    "# Create Feature Map\n",
    "feature_list = significant_features\n",
    "feature_map = {sig: i for i, sig in enumerate(feature_list)}\n",
    "print(f\"\\nSELECTED FEATURES: {len(feature_list)}\")\n",
    "\n",
    "# Save the signals (Task 2 Requirement)\n",
    "df_signals = pd.DataFrame(feature_list, columns=['junction_aa', 'v_call', 'j_call'])\n",
    "df_signals.to_csv(f\"dataset_{DATASET_NUM}_top_signals.csv\", index=False)\n",
    "print(f\"Signals saved to 'dataset_{DATASET_NUM}_top_signals.csv'\")\n",
    "\n",
    "# --- STEP 4: BUILD TRAINING MATRIX ---\n",
    "print(\"Step 4: Building Sparse Training Matrix...\")\n",
    "def create_sparse_matrix(filenames, base_dir):\n",
    "    n_samples = len(filenames)\n",
    "    n_features = len(feature_map)\n",
    "    X = sparse.lil_matrix((n_samples, n_features), dtype=np.int8)\n",
    "    \n",
    "    for row_idx, fname in enumerate(tqdm(filenames)):\n",
    "        sigs = load_signatures(base_dir / fname)\n",
    "        for s in sigs:\n",
    "            if s in feature_map:\n",
    "                X[row_idx, feature_map[s]] = 1\n",
    "    return X.tocsr()\n",
    "\n",
    "X_train = create_sparse_matrix(df_meta['filename'], TRAIN_DIR)\n",
    "y_train = df_meta['label_positive'].astype(int).values\n",
    "\n",
    "# --- STEP 5: TRAIN FINAL MODEL ---\n",
    "print(\"Step 5: Training Lasso Logistic Regression...\")\n",
    "# Use C=1.0 as default. If D1/D2 are noisy, lower C to 0.1\n",
    "clf = LogisticRegression(\n",
    "    penalty='l1', \n",
    "    solver='liblinear', \n",
    "    class_weight='balanced', \n",
    "    C=1.0, \n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Model Trained. Active Features: {np.sum(clf.coef_ != 0)}\")\n",
    "\n",
    "# --- STEP 6: PREDICT ON TEST SET ---\n",
    "print(\"Step 6: Processing Test Data...\")\n",
    "test_files = sorted([f.name for f in TEST_DIR.glob(\"*.tsv\")])\n",
    "if len(test_files) == 0:\n",
    "    print(\"WARNING: No test files found! Check TEST_DIR path.\")\n",
    "else:\n",
    "    print(f\"Found {len(test_files)} test files.\")\n",
    "    \n",
    "    # Build Test Matrix (Using same feature map)\n",
    "    X_test = create_sparse_matrix(test_files, TEST_DIR)\n",
    "    \n",
    "    # Predict\n",
    "    probs = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Create Submission (strip .tsv extension)\n",
    "    submission = pd.DataFrame({\n",
    "        'filename': [f.replace('.tsv', '') for f in test_files],\n",
    "        'probability': probs\n",
    "    })\n",
    "    \n",
    "    sub_filename = f\"dataset_{DATASET_NUM}_submission.csv\"\n",
    "    submission.to_csv(sub_filename, index=False)\n",
    "    print(f\"\\nSUCCESS! Predictions saved to '{sub_filename}'\")\n",
    "    print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8bb046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00bf3e8b5c44db7f9765a2c896308ca8</td>\n",
       "      <td>0.151802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00c0a5b0b5780839679ddcede9da56df</td>\n",
       "      <td>0.236472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00d82591a1ef87f94bc28b47f9a6c9a2</td>\n",
       "      <td>0.632017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>016b2ba917244113cdc4dd790c5ff3c7</td>\n",
       "      <td>0.685355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02dd6983048af267517bc0a57d53e1b0</td>\n",
       "      <td>0.967874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>fe47d786705b124ee75e76dd970e7290</td>\n",
       "      <td>0.151802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>feeae51e9d8974bc5048175d2300cfac</td>\n",
       "      <td>0.522939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>fef053aa6ddabf5d3c7f4c18f9b08a99</td>\n",
       "      <td>0.151802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>ff42cbc3605d26e8905cb98f19986321</td>\n",
       "      <td>0.202817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>ff909b16caa56f22e639950969df8d3e</td>\n",
       "      <td>0.520002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             filename  probability\n",
       "0    00bf3e8b5c44db7f9765a2c896308ca8     0.151802\n",
       "1    00c0a5b0b5780839679ddcede9da56df     0.236472\n",
       "2    00d82591a1ef87f94bc28b47f9a6c9a2     0.632017\n",
       "3    016b2ba917244113cdc4dd790c5ff3c7     0.685355\n",
       "4    02dd6983048af267517bc0a57d53e1b0     0.967874\n",
       "..                                ...          ...\n",
       "395  fe47d786705b124ee75e76dd970e7290     0.151802\n",
       "396  feeae51e9d8974bc5048175d2300cfac     0.522939\n",
       "397  fef053aa6ddabf5d3c7f4c18f9b08a99     0.151802\n",
       "398  ff42cbc3605d26e8905cb98f19986321     0.202817\n",
       "399  ff909b16caa56f22e639950969df8d3e     0.520002\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b0a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airr-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
